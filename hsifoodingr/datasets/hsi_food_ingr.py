from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from typing import Any, Callable, Dict, Literal, Optional, Sequence

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset


NormalizeMode = Literal["none", "minmax", "standard", "wavelength"]


@dataclass
class _MetadataCache:
    image_basenames: Optional[np.ndarray] = None
    dish_labels: Optional[np.ndarray] = None
    ingredient_map: Optional[dict] = None
    wavelengths: Optional[np.ndarray] = None


class HSIFoodIngrDataset(Dataset):
    """Dataset for HSIFoodIngr HDF5 files generated by HSIFoodIngr-Compression.

    This implementation follows the plan in `.cursor/rules/plan-dataset.mdc`.

    Returns a dictionary per sample:
        {
            "hsi": FloatTensor of shape (B, H, W),
            "rgb": FloatTensor (0..1) of shape (3, H, W) or UInt8Tensor (0..255),
            "mask": LongTensor of shape (H, W),
            "meta": { ... }
        }
    """

    def __init__(
        self,
        h5_path: str,
        normalize: NormalizeMode = "none",
        return_uint8_rgb: bool = False,
        transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
        cache_metadata: bool = True,
        strict: bool = True,
        band_indices: Optional[Sequence[int]] = None,
        wavelengths_nm: Optional[Sequence[float]] = None,
    ) -> None:
        super().__init__()
        self.h5_path = h5_path
        self.normalize: NormalizeMode = normalize
        self.return_uint8_rgb = return_uint8_rgb
        self.transform = transform
        self.cache_metadata = cache_metadata
        self.strict = strict
        # Band selection
        self._selected_order: Optional[np.ndarray] = None  # original order, may contain duplicates
        self._selected_unique_sorted: Optional[np.ndarray] = None  # unique ascending for efficient read
        self._selected_reorder_idx: Optional[np.ndarray] = None  # map unique->original order indices

        self._file: Optional[h5py.File] = None  # opened lazily per process/worker
        self._metadata_cache = _MetadataCache()

        # Light validation without reading the whole file
        self._validate_structure_light()
        # Compute band selection indices if requested
        self._init_band_selection(band_indices=band_indices, wavelengths_nm=wavelengths_nm)
        logging.getLogger(__name__).info(
            "Initialized HSIFoodIngrDataset: path=%s normalize=%s uint8_rgb=%s",
            h5_path,
            normalize,
            return_uint8_rgb,
        )

    # --- h5py lifecycle per worker/process ---
    def _ensure_file(self) -> h5py.File:
        if self._file is None:
            # Avoid sharing file handles across workers; open lazily
            self._file = h5py.File(self.h5_path, "r")
        return self._file

    def __del__(self) -> None:
        try:
            if self._file is not None:
                self._file.close()
        except Exception:
            pass

    def __getstate__(self):
        state = self.__dict__.copy()
        # h5py.File is not picklable/shareable. Drop it on fork; reopen in worker.
        state["_file"] = None
        return state

    # --- structure helpers ---
    @staticmethod
    def _require_group(f: h5py.File, key: str) -> h5py.Group | h5py.Dataset:
        if key not in f:
            raise KeyError(f"Missing key in HDF5: {key}")
        return f[key]

    def _validate_structure_light(self) -> None:
        with h5py.File(self.h5_path, "r") as f:
            # existence
            for key in ["/hsi", "/rgb", "/masks", "/metadata"]:
                if key not in f:
                    raise KeyError(f"Missing key in HDF5: {key}")
            # shapes/dtypes minimal check
            hsi = f["/hsi"]
            rgb = f["/rgb"]
            masks = f["/masks"]

            if hsi.ndim != 4:
                raise ValueError(f"/hsi must be 4D (N,H,W,B), got {hsi.shape}")
            if rgb.ndim != 4 or rgb.shape[-1] != 3:
                raise ValueError(f"/rgb must be 4D (N,H,W,3), got {rgb.shape}")
            if masks.ndim != 3:
                raise ValueError(f"/masks must be 3D (N,H,W), got {masks.shape}")

            self._num_samples = int(hsi.shape[0])

            # cache global metadata if requested
            if self.cache_metadata:
                md = f["/metadata"]
                if "ingredient_map" in md:
                    try:
                        raw = md["ingredient_map"][()]
                        if isinstance(raw, bytes):
                            raw = raw.decode("utf-8")
                        # vlen str (1,) or scalar; normalize to str
                        if isinstance(raw, np.ndarray) and raw.ndim >= 1:
                            raw = raw.reshape(-1)[0]
                            if isinstance(raw, bytes):
                                raw = raw.decode("utf-8")
                        self._metadata_cache.ingredient_map = json.loads(str(raw))
                    except Exception as exc:
                        if self.strict:
                            raise ValueError("Invalid /metadata/ingredient_map JSON") from exc
                if "wavelengths" in md:
                    self._metadata_cache.wavelengths = np.asarray(md["wavelengths"], dtype=np.float32)

    # --- band selection helpers ---
    def _init_band_selection(
        self,
        band_indices: Optional[Sequence[int]],
        wavelengths_nm: Optional[Sequence[float]],
    ) -> None:
        if (band_indices is None or len(band_indices) == 0) and (
            wavelengths_nm is None or len(wavelengths_nm) == 0
        ):
            # No selection â†’ use all bands
            self._selected_order = None
            self._selected_unique_sorted = None
            self._selected_reorder_idx = None
            return

        # Prefer explicit band indices
        if band_indices is not None and len(band_indices) > 0:
            order = np.asarray(list(band_indices), dtype=int)
        else:
            # Map wavelengths to nearest band indices
            if self._metadata_cache.wavelengths is None:
                if self.strict:
                    raise ValueError("/metadata/wavelengths not available to resolve wavelengths_nm")
                # Fallback: disable selection
                self._selected_order = None
                self._selected_unique_sorted = None
                self._selected_reorder_idx = None
                return
            wl = self._metadata_cache.wavelengths.astype(np.float32)
            targets = np.asarray(list(wavelengths_nm or []), dtype=np.float32)
            order = np.asarray([int(np.argmin(np.abs(wl - t))) for t in targets], dtype=int)

        # Validate range
        with h5py.File(self.h5_path, "r") as f:
            B = int(f["/hsi"].shape[-1])
        if order.size == 0:
            raise ValueError("Selected band list is empty")
        if np.any(order < 0) or np.any(order >= B):
            raise ValueError(f"Selected band index out of range [0,{B-1}]: {order}")

        # Build unique sorted and reorder indices
        unique_sorted = np.unique(order)
        # Map band value -> position in unique_sorted
        pos = {int(v): i for i, v in enumerate(unique_sorted.tolist())}
        reorder_idx = np.asarray([pos[int(v)] for v in order.tolist()], dtype=int)

        self._selected_order = order  # original possibly with duplicates
        self._selected_unique_sorted = unique_sorted
        self._selected_reorder_idx = reorder_idx

    # --- core dataset protocol ---
    def __len__(self) -> int:  # type: ignore[override]
        return self._num_samples

    def __getitem__(self, index: int) -> Dict[str, Any]:  # type: ignore[override]
        if not (0 <= index < self._num_samples):
            raise IndexError(f"index out of range: {index}")

        f = self._ensure_file()

        # Read arrays lazily for this index
        # HSI: optionally select bands at read time for efficiency
        if self._selected_unique_sorted is not None:
            try:
                # h5py fancy indexing along last axis
                hsi_np = np.asarray(
                    f["/hsi"][index, :, :, self._selected_unique_sorted], dtype=np.float32
                )  # (H,W,B_sel_unique)
                # Reorder and duplicate channels to match original requested order
                if self._selected_reorder_idx is not None and (
                    self._selected_reorder_idx.size != self._selected_unique_sorted.size or not np.all(
                        self._selected_reorder_idx == np.arange(self._selected_reorder_idx.size)
                    )
                ):
                    hsi_np = hsi_np[..., self._selected_reorder_idx]
            except Exception:
                # Fallback: read full and slice with numpy
                full_np = np.asarray(f["/hsi"][index], dtype=np.float32)
                hsi_np = full_np[..., self._selected_order]
        else:
            hsi_np = np.asarray(f["/hsi"][index], dtype=np.float32)  # (H,W,B)
        rgb_np = np.asarray(f["/rgb"][index])  # uint8 (H,W,3)
        mask_np = np.asarray(f["/masks"][index])  # int (H,W)

        # Sanity checks
        if hsi_np.ndim != 3:
            raise ValueError(f"/hsi[{index}] must be 3D (H,W,B), got {hsi_np.shape}")
        if rgb_np.ndim != 3 or rgb_np.shape[-1] != 3:
            raise ValueError(f"/rgb[{index}] must be 3D (H,W,3), got {rgb_np.shape}")
        if mask_np.ndim != 2:
            raise ValueError(f"/masks[{index}] must be 2D (H,W), got {mask_np.shape}")

        # Convert to tensors with channel-first where applicable
        hsi_t = torch.from_numpy(np.moveaxis(hsi_np, -1, 0).copy()).to(torch.float32)
        if self.return_uint8_rgb:
            rgb_t = torch.from_numpy(np.moveaxis(rgb_np, -1, 0).copy()).to(torch.uint8)
        else:
            rgb_t = torch.from_numpy(np.moveaxis(rgb_np, -1, 0).astype(np.float32) / 255.0)
        mask_t = torch.from_numpy(mask_np.astype(np.int64))

        # Normalization for HSI
        if self.normalize != "none":
            hsi_t = self._normalize_hsi(hsi_t)

        meta = self._build_sample_meta(f, index)

        sample: Dict[str, Any] = {
            "hsi": hsi_t,
            "rgb": rgb_t,
            "mask": mask_t,
            "meta": meta,
        }

        if self.transform is not None:
            sample = self.transform(sample)

        return sample

    # --- normalization ---
    @staticmethod
    def _safe_minmax(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        # compute per-channel min and max with eps for stability
        reduce_axes = tuple(range(1, x.ndim))  # all dims except channel
        x_min = x.amin(dim=reduce_axes, keepdim=True)
        x_max = x.amax(dim=reduce_axes, keepdim=True)
        return x_min, x_max

    def _normalize_hsi(self, x: torch.Tensor) -> torch.Tensor:
        if self.normalize == "minmax":
            x_min, x_max = self._safe_minmax(x)
            eps = 1e-6
            return (x - x_min) / (x_max - x_min + eps)
        if self.normalize == "standard":
            reduce_axes = tuple(range(1, x.ndim))
            mean = x.mean(dim=reduce_axes, keepdim=True)
            std = x.std(dim=reduce_axes, keepdim=True).clamp_min(1e-6)
            return (x - mean) / std
        if self.normalize == "wavelength":
            # Placeholder: hook for wavelength-aware normalization
            # For now, identical to "none" to avoid accidental distortion.
            return x
        return x

    # --- metadata helpers ---
    def _build_sample_meta(self, f: h5py.File, index: int) -> Dict[str, Any]:
        md: Dict[str, Any] = {}
        meta_group = f["/metadata"] if "/metadata" in f else None

        # Image basenames
        try:
            if (
                self._metadata_cache.image_basenames is None
                and meta_group is not None
                and "image_basenames" in meta_group
            ):
                # Decode as Python str using h5py's asstr helper when available
                ds = meta_group["image_basenames"]
                try:
                    arr = ds.asstr()[...]
                except Exception:
                    arr = np.asarray(ds[...])
                    if arr.dtype.kind == "S":
                        arr = arr.astype("U")
                    elif arr.dtype == object:
                        arr = np.array([x.decode("utf-8") if isinstance(x, (bytes, bytearray)) else str(x) for x in arr])
                self._metadata_cache.image_basenames = arr
            if self._metadata_cache.image_basenames is not None:
                md["basename"] = str(self._metadata_cache.image_basenames[index])
        except Exception:
            if self.strict:
                raise

        # Dish labels
        try:
            if (
                self._metadata_cache.dish_labels is None
                and meta_group is not None
                and "dish_labels" in meta_group
            ):
                ds = meta_group["dish_labels"]
                try:
                    arr = ds.asstr()[...]
                except Exception:
                    arr = np.asarray(ds[...])
                    if arr.dtype.kind == "S":
                        arr = arr.astype("U")
                    elif arr.dtype == object:
                        arr = np.array([x.decode("utf-8") if isinstance(x, (bytes, bytearray)) else str(x) for x in arr])
                self._metadata_cache.dish_labels = arr
            if self._metadata_cache.dish_labels is not None:
                md["dish_label"] = str(self._metadata_cache.dish_labels[index])
        except Exception:
            if self.strict:
                raise

        # Ingredient map and wavelengths are global
        if self._metadata_cache.ingredient_map is not None:
            md["ingredient_map"] = self._metadata_cache.ingredient_map
        if self._metadata_cache.wavelengths is not None:
            md["wavelengths"] = self._metadata_cache.wavelengths

        return md
